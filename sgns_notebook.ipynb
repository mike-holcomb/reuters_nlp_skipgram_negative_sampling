{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Skipgram with Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mike Holcomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.datasets import reuters\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "c = 8 # Context window size (+8 t -8)\n",
    "K = 5 # Number of negative samples per positive example\n",
    "batch_size = 16384\n",
    "embedding_size = 100\n",
    "train_steps = 5000\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corpus\n",
    "\n",
    "\n",
    "(train, _), (test, _) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=vocab_size,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = reuters.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.concatenate((train,test))\n",
    "num_articles = corpus.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gather Unigram Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Per section 2.2 Negative sampling**\n",
    "1. Generate frequency table\n",
    "2. Alpha smooth highest frequencies\n",
    "3. get k noise words sampling from alpha smoothed distribution\n",
    "4. Add k noise words as negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11228/11228 [00:00<00:00, 17148.45it/s]\n"
     ]
    }
   ],
   "source": [
    "freq = np.zeros((vocab_size,))\n",
    "for i in tqdm(range(num_articles)):\n",
    "    for w in corpus[i]:\n",
    "        freq[w] += 1\n",
    "\n",
    "raw_prob = freq / np.sum(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha smooth the unigram distribution\n",
    "alpha = 0.75\n",
    "alpha_counts = np.power(freq, alpha)\n",
    "alpha_counts[0:4] = 0 # Don't include symbols as noise words\n",
    "alpha_prob = alpha_counts / np.sum(alpha_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_noise_words = lambda k : np.argmax(np.random.multinomial(1, alpha_prob,k),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Per section 2.3 Subsampling of frequent words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 4.2e-6 # Goal seek such that smallest discard probability is ~0; originally 1e-5 in paper\n",
    "discard_prob = 1. - np.sqrt(t * np.reciprocal(np.where(raw_prob == 0, 1., raw_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_pairs = [] # Create list of target-context pair tuples\n",
    "targets = [] # Create list of target; Set positive := 1, negative := 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_example(t, c):\n",
    "    \"\"\"\n",
    "    Add a positive example and K negative examples\n",
    "    :param t: target word\n",
    "    :param c: context word\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if t == c:\n",
    "        return\n",
    "    ct_pairs.append([t, c])\n",
    "    targets.append(1)\n",
    "\n",
    "    noise_words = get_noise_words(K)\n",
    "    for noise in noise_words:\n",
    "        if t == noise:\n",
    "            noise = noise + 1\n",
    "        ct_pairs.append([t, noise])\n",
    "        targets.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11228/11228 [21:48<00:00,  8.70it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(num_articles)): # num_articles\n",
    "    article = corpus[i]\n",
    "    for j in range(len(article) - c - 1):\n",
    "        w = article[j]\n",
    "        if w < 4:\n",
    "            continue # Skip symbols\n",
    "\n",
    "        for k in range(1,c+1):\n",
    "            v = article[j + k]\n",
    "            if v < 4: # Skip symbols\n",
    "                continue\n",
    "\n",
    "            if random.random() > discard_prob[w]:\n",
    "                add_example(w, v)\n",
    "\n",
    "            if random.random() > discard_prob[v]:\n",
    "                add_example(v, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgns_fn(features, labels, mode, params):\n",
    "    \"\"\"\n",
    "    Model function implementing Skip Gram with Negative Sampling\n",
    "    \"\"\"\n",
    "\n",
    "    # target word vector\n",
    "    u = tf.feature_column.input_layer(features,[params['feature_columns'][0] ])\n",
    "    \n",
    "    # context word vector\n",
    "    v = tf.feature_column.input_layer(features, [params['feature_columns'][1] ])\n",
    "\n",
    "    # dot product similarity\n",
    "    z = tf.reduce_sum(tf.multiply(u, v), axis=1)\n",
    "\n",
    "    # compute likelihood of being in the same context\n",
    "    predictions = tf.sigmoid(z)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predicts={\"in_context\": predictions}\n",
    "        )\n",
    "\n",
    "    # compute the cross entropy between the vectors = negative log likelihood\n",
    "    total_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels,logits=z)\n",
    "    average_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = params.get(\"optimizer\", tf.train.AdamOptimizer)\n",
    "        optimizer = optimizer(params.get(\"learning_rate\", 1e-3))\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=average_loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, loss=average_loss, train_op=train_op)\n",
    "\n",
    "    assert mode == tf.estimator.ModeKeys.EVAL\n",
    "\n",
    "    # Calculate predictions\n",
    "    print(labels)\n",
    "    print(predictions)\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        # Report sum of error for compatibility with pre-made estimators\n",
    "        loss=average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = np.array(ct_pairs)\n",
    "words = pairs[:,0]\n",
    "contexts = pairs[:,1]\n",
    "labels = np.array(targets,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(batch_sz, w,c , y=None, shuffle=False, shuffle_buffer_size=1000):\n",
    "    \"\"\"Create a slice Dataset from a pandas DataFrame and labels\"\"\"\n",
    "    features = {'words': w, 'contexts': c}\n",
    "\n",
    "    def input_fn():\n",
    "        if y is not None:\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((features , y))\n",
    "        else:\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((features, ))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(shuffle_buffer_size).batch(batch_sz, drop_remainder=True).repeat()\n",
    "        else:\n",
    "            dataset = dataset.batch(batch_sz, drop_remainder=True)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = make_dataset(batch_size, words, contexts, labels,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_feat = tf.feature_column.categorical_column_with_identity(key=\"words\",\n",
    "                                                                    num_buckets=vocab_size)\n",
    "\n",
    "context_feat = tf.feature_column.categorical_column_with_identity(key=\"contexts\",\n",
    "                                                                    num_buckets=vocab_size)\n",
    "\n",
    "words_embed = tf.feature_column.embedding_column(words_feat,\n",
    "                                                     dimension=embedding_size,\n",
    "                                                     trainable=True)\n",
    "\n",
    "context_embed = tf.feature_column.embedding_column(context_feat,\n",
    "                                                     dimension=embedding_size,\n",
    "                                                     trainable=True)\n",
    "\n",
    "feature_columns = [\n",
    "    words_embed,\n",
    "    context_embed\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    \"feature_columns\" : feature_columns,\n",
    "    \"learning_rate\" : learning_rate,\n",
    "    \"optimizer\" : tf.train.AdamOptimizer,\n",
    "    \"embedding_size\" : embedding_size,\n",
    "    \"vocab_size\" : vocab_size\n",
    "}\n",
    "\n",
    "config = tf.estimator.RunConfig(\n",
    "    model_dir='./estimator_model',\n",
    "    tf_random_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './estimator_model', '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xab5873ac8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(\n",
    "    model_fn = sgns_fn,\n",
    "    params = p,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./estimator_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.69382256, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1.85376\n",
      "INFO:tensorflow:loss = 0.6938143, step = 101 (53.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1581\n",
      "INFO:tensorflow:loss = 0.69337523, step = 201 (9.840 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4696\n",
      "INFO:tensorflow:loss = 0.69380945, step = 301 (9.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5643\n",
      "INFO:tensorflow:loss = 0.6937982, step = 401 (9.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8998\n",
      "INFO:tensorflow:loss = 0.69380057, step = 501 (9.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.31897\n",
      "INFO:tensorflow:loss = 0.6932485, step = 601 (10.730 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7126\n",
      "INFO:tensorflow:loss = 0.6931206, step = 701 (9.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3166\n",
      "INFO:tensorflow:loss = 0.6929274, step = 801 (8.831 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1273\n",
      "INFO:tensorflow:loss = 0.6925378, step = 901 (8.987 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9492\n",
      "INFO:tensorflow:loss = 0.6920464, step = 1001 (9.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0168\n",
      "INFO:tensorflow:loss = 0.69160706, step = 1101 (9.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.01\n",
      "INFO:tensorflow:loss = 0.6905523, step = 1201 (9.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2867\n",
      "INFO:tensorflow:loss = 0.6895665, step = 1301 (8.860 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2326\n",
      "INFO:tensorflow:loss = 0.6887787, step = 1401 (8.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4599\n",
      "INFO:tensorflow:loss = 0.68727696, step = 1501 (8.726 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3146\n",
      "INFO:tensorflow:loss = 0.68515664, step = 1601 (8.838 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2752\n",
      "INFO:tensorflow:loss = 0.68354833, step = 1701 (8.869 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4338\n",
      "INFO:tensorflow:loss = 0.68206143, step = 1801 (8.746 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4003\n",
      "INFO:tensorflow:loss = 0.6780833, step = 1901 (8.772 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4457\n",
      "INFO:tensorflow:loss = 0.67478025, step = 2001 (8.737 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5711\n",
      "INFO:tensorflow:loss = 0.6720091, step = 2101 (8.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4094\n",
      "INFO:tensorflow:loss = 0.6673762, step = 2201 (8.765 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5157\n",
      "INFO:tensorflow:loss = 0.6585535, step = 2301 (8.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2114\n",
      "INFO:tensorflow:loss = 0.6552785, step = 2401 (8.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2881\n",
      "INFO:tensorflow:loss = 0.64738977, step = 2501 (8.859 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0883\n",
      "INFO:tensorflow:loss = 0.64187336, step = 2601 (9.018 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2706\n",
      "INFO:tensorflow:loss = 0.63373655, step = 2701 (8.873 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5403\n",
      "INFO:tensorflow:loss = 0.62770057, step = 2801 (8.665 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3821\n",
      "INFO:tensorflow:loss = 0.62075984, step = 2901 (8.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4028\n",
      "INFO:tensorflow:loss = 0.618512, step = 3001 (8.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1693\n",
      "INFO:tensorflow:loss = 0.6049544, step = 3101 (8.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0093\n",
      "INFO:tensorflow:loss = 0.5956211, step = 3201 (9.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2739\n",
      "INFO:tensorflow:loss = 0.59136486, step = 3301 (8.870 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3504\n",
      "INFO:tensorflow:loss = 0.581856, step = 3401 (8.810 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4819\n",
      "INFO:tensorflow:loss = 0.57363033, step = 3501 (8.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5259\n",
      "INFO:tensorflow:loss = 0.57129985, step = 3601 (8.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4812\n",
      "INFO:tensorflow:loss = 0.56483746, step = 3701 (8.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4948\n",
      "INFO:tensorflow:loss = 0.56011534, step = 3801 (8.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5659\n",
      "INFO:tensorflow:loss = 0.5565232, step = 3901 (8.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2775\n",
      "INFO:tensorflow:loss = 0.54349613, step = 4001 (8.867 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2744\n",
      "INFO:tensorflow:loss = 0.5467527, step = 4101 (8.869 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4199\n",
      "INFO:tensorflow:loss = 0.54361445, step = 4201 (8.756 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3139\n",
      "INFO:tensorflow:loss = 0.53224623, step = 4301 (8.839 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1893\n",
      "INFO:tensorflow:loss = 0.52965736, step = 4401 (8.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1809\n",
      "INFO:tensorflow:loss = 0.52388316, step = 4501 (8.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2181\n",
      "INFO:tensorflow:loss = 0.5211041, step = 4601 (8.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3664\n",
      "INFO:tensorflow:loss = 0.52493656, step = 4701 (8.798 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.571\n",
      "INFO:tensorflow:loss = 0.5176519, step = 4801 (8.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1731\n",
      "INFO:tensorflow:loss = 0.5127243, step = 4901 (8.950 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into ./estimator_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5108459.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0xab58737b8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn=train_input_fn, steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel Python",
   "language": "python",
   "name": "intelpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
